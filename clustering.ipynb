{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0502c2cd614d093a3be30031cbabda9e1b1b1c5d2fdc17ac0be9ebae842ffa0cd",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering Individual Household Electric Power Consumption and Future Consumption Regression Analysis.\n",
    "\n",
    "Our group proposes to use the Individual household electric power consumption data set to look for power consumption trends over time. We plan on clustering the data using descriptive methods to discover patterns and trends. Applying predictive methods such as regression we plan to predict future power consumption.\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cityblock\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "source": [
    "# Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Process and clean the data\n",
    "Process the data by reading each line, removing the column header information and stripping the semicolon seperators. Then convert the date and time stamps to numeric values and merge the two to have a dataset with all numeric values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_ratio(time_stamp):\n",
    "    time = datetime.strptime(time_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    start = datetime(year=time.year, month=1, day=1)\n",
    "    end = datetime(year=time.year+1, month=1, day=1)\n",
    "    return (time - start).total_seconds()/(end - start).total_seconds()\n",
    "\n",
    "def minutes_from_start(time_stamp, start_stamp):\n",
    "    time = datetime.strptime(time_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    start = datetime.strptime(start_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    return (time - start).total_seconds()/60.0\n",
    "\n",
    "def get_cluster_number(labels):\n",
    "    return len(set(labels)) - (1 if -1 in labels else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from text document\n",
    "with open('household_power_consumption.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "# Remove the '?' uncaptured data if detected\n",
    "data_raw_reduced = [line for line in lines if '?' not in line] \n",
    "\n",
    "# strip the header information and remove semicolons     \n",
    "data_raw = [l.split(';') for l in data_raw_reduced][1::]\n",
    "\n",
    "X_list = [[float(d[6]), float(d[7]), float(d[8])] for d in data_raw]"
   ]
  },
  {
   "source": [
    "## Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_list)\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] *= (1.0/X[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = f'{data_raw[0][0]} {data_raw[0][1]}'\n",
    "N = 1000\n",
    "M = 5000\n",
    "step = 10\n",
    "time_from_start = [minutes_from_start(f'{d[0]} {d[1]}', start_time) for d in data_raw[N:M:step]]\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(time_from_start, X[N:M:step,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = X[::5].copy()\n",
    "\n",
    "X_red = X_red[~np.all(X_red == 0.0, axis=1)]\n",
    "print(X_red.shape[0])\n",
    "X_train, X_test = train_test_split(X_red, train_size=0.75, random_state=42)\n",
    "print(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis = []\n",
    "# for row in X:\n",
    "#     dis.append(cityblock(row, X[0]))\n",
    "# dis = np.array(dis)\n",
    "# dis = dis[dis < 0.02]\n",
    "# print(dis.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X_train[:,0], X_train[:,1], X_train[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Cluster Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan = DBSCAN(eps=0.025, min_samples=200, metric='manhattan', algorithm='ball_tree', leaf_size=10000)\n",
    "# dbscan.fit(X_red)\n",
    "# labels = dbscan.labels_\n",
    "\n",
    "optics = OPTICS(min_samples=200, max_eps=0.5, metric='manhattan', min_cluster_size=0.01,  n_jobs=-1, cluster_method='dbscan', eps=0.05)\n",
    "optics.fit(X_train)\n",
    "labels = optics.labels_\n",
    "\n",
    "print(get_cluster_number(labels))\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "for l in set(labels):\n",
    "    X_bylabel = X_train[labels == l]\n",
    "    ax.scatter3D(X_bylabel[:,0], X_bylabel[:,1], X_bylabel[:,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in set(labels):\n",
    "    print(f'Number of points in cluster {l}: {labels[labels == l].size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'minsamp200maxeps0.5minclust0.01eps0.05.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}