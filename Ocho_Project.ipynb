{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0502c2cd614d093a3be30031cbabda9e1b1b1c5d2fdc17ac0be9ebae842ffa0cd",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clustering Individual Household Electric Power Consumption and Future Consumption Regression Analysis.\n",
    "\n",
    "Our group proposes to use the Individual household electric power consumption data set to look for power consumption trends over time. We plan on clustering the data using descriptive methods to discover patterns and trends. Applying predictive methods such as regression we plan to predict future power consumption.\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "source": [
    "# Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Process and clean the data\n",
    "Process the data by reading each line, removing the column header information and stripping the semicolon seperators. Then convert the date and time stamps to numeric values and merge the two to have a dataset with all numeric values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_ratio(time_stamp):\n",
    "    time = datetime.strptime(time_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    start = datetime(year=time.year, month=1, day=1)\n",
    "    end = datetime(year=time.year+1, month=1, day=1)\n",
    "    return (time - start).total_seconds()/(end - start).total_seconds()\n",
    "\n",
    "def minutes_from_start(time_stamp, start_stamp):\n",
    "    time = datetime.strptime(time_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    start = datetime.strptime(start_stamp, '%d/%m/%Y %H:%M:%S')\n",
    "    return (time - start).total_seconds()/60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from text document\n",
    "with open('household_power_consumption.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "# Remove the '?' uncaptured data if detected\n",
    "data_raw_reduced = [line for line in lines if '?' not in line] \n",
    "\n",
    "# strip the header information and remove semicolons     \n",
    "data_raw = [l.split(';') for l in data_raw_reduced][1::]\n",
    "\n",
    "# Convert date and time to a numeric value/ratio\n",
    "time_ratios = [time_to_ratio(f'{t[0]} {t[1]}') for t in data_raw]\n",
    "\n",
    "# merge time with raw data removing time stamp strings and replacing with ratios\n",
    "data_time_raw = [[t, float(gap), float(grp), float(v), float(gi), float(s1), float(s2), float(s3)] for (_, _, gap, grp, v, gi, s1, s2, s3), (t) in zip(data_raw, time_ratios)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify columns/rows/data are as expected.\n",
    "print(\"Number of rows: {}\".format(len(data_time_raw)))\n",
    "print(\"Number of columns: {}\".format(len(data_time_raw[0])))\n",
    "print(data_time_raw[:10])\n",
    "\n",
    "# Convert to np array for better processing.\n",
    "data_time_np = np.array(data_time_raw, dtype=float)\n",
    "print(\"Number of rows: {}\".format(data_time_np.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data_time_np.shape[1]))\n",
    "print(data_time_np[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = f'{data_raw[0][0]} {data_raw[0][1]}' # the first time stamp\n",
    "time_from_start = [minutes_from_start(f'{t[0]} {t[1]}', start_time) for t in data_raw] # array of minutes from first time stamp"
   ]
  },
  {
   "source": [
    "## Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional Preprocessing Steps here ##\n",
    "global_power = data_time_np[:,1].copy() # global power - to check trends\n",
    "\n",
    "# Normalize by max value in a column\n",
    "for i in range(1, data_time_np.shape[1]):\n",
    "    data_time_np[:,i] *= (1.0/data_time_np[:,i].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(time_from_start, global_power)\n",
    "plt.figure(2)\n",
    "plt.plot(time_from_start, data_time_np[:,1])\n",
    "\n",
    "print(data_time_np[:10])"
   ]
  },
  {
   "source": [
    "## Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "data_time_np_reduced = pca.fit_transform(data_time_np)\n",
    "\n",
    "print(data_time_np_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained Variance from 4 components: {pca.explained_variance_ratio_.sum()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, figsize=(15,30))\n",
    "for i in range(8):\n",
    "    axs[i].plot(time_from_start, data_time_np[:,i])\n",
    "    axs[i].set_title(f'feature {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_time_np_reduced = np.delete(data_time_np, [1,2,3,4], axis=1)\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(15,30))\n",
    "for i in range(4):\n",
    "    axs[i].plot(time_from_start, data_time_np_reduced[:,i])\n",
    "    axs[i].set_title(f'feature {i}')"
   ]
  },
  {
   "source": [
    "# Cluster Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}